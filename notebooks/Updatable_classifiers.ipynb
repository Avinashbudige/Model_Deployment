{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e900712",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0b482b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96976fe2",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aaae8f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris() #loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1da8317",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = data['data'],data['target'] #spliting the data into features and target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d444154",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8918a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MinMaxScaler \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "minmax_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "minmax_scaler.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27c902b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting the X data in minmaxscaler\n",
    "X=minmax_scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e29c9b",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fe58b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "train, test, labels_train, labels_test = train_test_split(X, y, train_size=0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f4826",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.c_[train,labels_train]\n",
    "np.savetxt('iris_train.csv', D, delimiter=\",\") #saving the data into csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec19eec",
   "metadata": {},
   "source": [
    "# Model Prediction Using Chunck Size \n",
    "(Updating the weights by chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8399f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1         2         3    4\n",
      "0   0.333333  0.916667  0.067797  0.041667  0.0\n",
      "1   0.805556  0.666667  0.864407  1.000000  2.0\n",
      "2   0.611111  0.416667  0.762712  0.708333  2.0\n",
      "3   0.305556  0.416667  0.593220  0.583333  1.0\n",
      "4   0.083333  0.666667  0.000000  0.041667  0.0\n",
      "5   0.333333  0.250000  0.576271  0.458333  1.0\n",
      "6   0.222222  0.208333  0.338983  0.416667  1.0\n",
      "7   0.611111  0.333333  0.610169  0.583333  1.0\n",
      "8   0.166667  0.208333  0.593220  0.666667  2.0\n",
      "9   0.555556  0.125000  0.576271  0.500000  1.0\n",
      "10  0.500000  0.375000  0.627119  0.541667  1.0\n",
      "11  0.416667  0.291667  0.694915  0.750000  2.0\n",
      "12  0.027778  0.375000  0.067797  0.041667  0.0\n",
      "13  0.305556  0.708333  0.084746  0.041667  0.0\n",
      "14  0.027778  0.416667  0.050847  0.041667  0.0\n",
      "15  0.500000  0.333333  0.508475  0.500000  1.0\n",
      "16  0.250000  0.625000  0.084746  0.041667  0.0\n",
      "17  0.666667  0.416667  0.677966  0.666667  1.0\n",
      "18  0.138889  0.583333  0.152542  0.041667  0.0\n",
      "19  0.777778  0.416667  0.830508  0.833333  2.0\n",
      "20  0.138889  0.416667  0.067797  0.083333  0.0\n",
      "21  0.638889  0.375000  0.610169  0.500000  1.0\n",
      "22  0.388889  0.333333  0.525424  0.500000  1.0\n",
      "23  0.583333  0.291667  0.728814  0.750000  2.0\n",
      "24  0.250000  0.291667  0.491525  0.541667  1.0\n",
      "[[ -8.68055556   7.32421875 -12.08289195 -13.02083333]\n",
      " [  1.08506944 -12.61393229   5.95868644   3.66210938]\n",
      " [  0.81380208  -4.47591146   5.62764831   8.13802083]]\n",
      "[1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0.]\n",
      "0.8666666666666667\n",
      "           0         1         2         3    4\n",
      "25  0.555556  0.583333  0.779661  0.958333  2.0\n",
      "26  0.361111  0.416667  0.593220  0.583333  1.0\n",
      "27  0.861111  0.333333  0.864407  0.750000  2.0\n",
      "28  0.555556  0.208333  0.677966  0.750000  2.0\n",
      "29  0.305556  0.791667  0.118644  0.125000  0.0\n",
      "30  0.027778  0.500000  0.050847  0.041667  0.0\n",
      "31  0.555556  0.541667  0.627119  0.625000  1.0\n",
      "32  0.472222  0.375000  0.593220  0.583333  1.0\n",
      "33  0.194444  0.625000  0.050847  0.083333  0.0\n",
      "34  0.388889  0.416667  0.542373  0.458333  1.0\n",
      "35  0.305556  0.791667  0.050847  0.125000  0.0\n",
      "36  0.722222  0.458333  0.745763  0.833333  2.0\n",
      "37  0.750000  0.500000  0.627119  0.541667  1.0\n",
      "38  0.361111  0.333333  0.661017  0.791667  2.0\n",
      "39  0.444444  0.416667  0.542373  0.583333  1.0\n",
      "40  0.277778  0.708333  0.084746  0.041667  0.0\n",
      "41  1.000000  0.750000  0.915254  0.791667  2.0\n",
      "42  0.083333  0.583333  0.067797  0.083333  0.0\n",
      "43  0.194444  0.625000  0.101695  0.208333  0.0\n",
      "44  0.222222  0.541667  0.118644  0.166667  0.0\n",
      "45  0.250000  0.875000  0.084746  0.000000  0.0\n",
      "46  0.222222  0.750000  0.101695  0.041667  0.0\n",
      "47  0.333333  0.625000  0.050847  0.041667  0.0\n",
      "48  0.361111  0.291667  0.542373  0.500000  1.0\n",
      "49  0.666667  0.541667  0.796610  1.000000  2.0\n",
      "[[ -7.32421875   9.35872396 -15.39327331 -16.27604167]\n",
      " [ -1.89887153 -11.39322917  -0.16551907  -4.47591146]\n",
      " [ 11.66449653  -7.73111979  10.75873941  13.83463542]]\n",
      "[2. 0. 2. 0. 0. 2. 0. 0. 2. 2. 0. 2. 0. 0. 0.]\n",
      "0.6666666666666666\n",
      "           0         1         2         3    4\n",
      "50  0.472222  0.291667  0.694915  0.625000  1.0\n",
      "51  0.222222  0.583333  0.084746  0.041667  0.0\n",
      "52  0.555556  0.333333  0.694915  0.583333  2.0\n",
      "53  0.472222  0.583333  0.593220  0.625000  1.0\n",
      "54  0.527778  0.375000  0.559322  0.500000  1.0\n",
      "55  0.194444  0.541667  0.067797  0.041667  0.0\n",
      "56  0.805556  0.500000  0.847458  0.708333  2.0\n",
      "57  0.611111  0.416667  0.711864  0.791667  2.0\n",
      "58  0.166667  0.416667  0.067797  0.041667  0.0\n",
      "59  0.333333  0.166667  0.457627  0.375000  1.0\n",
      "60  0.166667  0.458333  0.084746  0.000000  0.0\n",
      "61  0.388889  0.250000  0.423729  0.375000  1.0\n",
      "62  0.472222  0.083333  0.508475  0.375000  1.0\n",
      "63  0.916667  0.416667  0.949153  0.833333  2.0\n",
      "64  0.694444  0.333333  0.644068  0.541667  1.0\n",
      "65  0.555556  0.208333  0.661017  0.583333  1.0\n",
      "66  0.416667  0.291667  0.525424  0.375000  1.0\n",
      "67  0.194444  0.583333  0.101695  0.125000  0.0\n",
      "68  0.472222  0.083333  0.677966  0.583333  2.0\n",
      "69  0.666667  0.458333  0.779661  0.958333  2.0\n",
      "70  0.666667  0.208333  0.813559  0.708333  2.0\n",
      "71  0.416667  0.833333  0.033898  0.041667  0.0\n",
      "72  0.138889  0.583333  0.101695  0.041667  0.0\n",
      "73  0.583333  0.500000  0.728814  0.916667  2.0\n",
      "74  0.166667  0.666667  0.067797  0.000000  0.0\n",
      "[[ -3.25520833   6.91731771  -9.765625    -8.95182292]\n",
      " [ -3.79774306 -12.20703125  -2.31726695  -5.69661458]\n",
      " [  6.23914931   2.84830729   9.10354873  13.42773437]]\n",
      "[2. 0. 2. 0. 0. 2. 2. 0. 2. 2. 0. 2. 0. 0. 0.]\n",
      "0.6666666666666666\n",
      "           0         1         2         3    4\n",
      "75  0.388889  1.000000  0.084746  0.125000  0.0\n",
      "76  0.194444  0.500000  0.033898  0.041667  0.0\n",
      "77  0.111111  0.500000  0.050847  0.041667  0.0\n",
      "78  0.583333  0.333333  0.779661  0.875000  2.0\n",
      "79  0.666667  0.541667  0.796610  0.833333  2.0\n",
      "80  0.722222  0.458333  0.694915  0.916667  2.0\n",
      "81  0.722222  0.458333  0.661017  0.583333  1.0\n",
      "82  0.527778  0.083333  0.593220  0.583333  1.0\n",
      "83  0.083333  0.500000  0.067797  0.041667  0.0\n",
      "84  0.555556  0.541667  0.847458  1.000000  2.0\n",
      "85  0.583333  0.333333  0.779661  0.833333  2.0\n",
      "86  0.222222  0.750000  0.152542  0.125000  0.0\n",
      "87  0.194444  0.666667  0.067797  0.041667  0.0\n",
      "88  0.583333  0.375000  0.559322  0.500000  1.0\n",
      "89  0.416667  0.250000  0.508475  0.458333  1.0\n",
      "90  0.833333  0.375000  0.898305  0.708333  2.0\n",
      "91  0.694444  0.416667  0.762712  0.833333  2.0\n",
      "92  0.194444  0.000000  0.423729  0.375000  1.0\n",
      "93  0.500000  0.250000  0.779661  0.541667  2.0\n",
      "94  0.944444  0.333333  0.966102  0.791667  2.0\n",
      "95  0.388889  0.750000  0.118644  0.083333  0.0\n",
      "96  0.388889  0.208333  0.677966  0.791667  2.0\n",
      "97  0.444444  0.416667  0.694915  0.708333  2.0\n",
      "98  0.333333  0.166667  0.474576  0.416667  1.0\n",
      "99  0.611111  0.416667  0.813559  0.875000  2.0\n",
      "[[-3.25520833e+00  8.54492188e+00 -1.20828919e+01 -1.30208333e+01]\n",
      " [ 2.17013889e+00 -1.42415365e+01 -1.82070975e+00 -5.69661458e+00]\n",
      " [ 5.15407986e+00  3.03273032e-15  1.48967161e+01  1.79036458e+01]]\n",
      "[2. 0. 2. 0. 0. 2. 2. 0. 2. 2. 0. 2. 0. 0. 0.]\n",
      "0.6666666666666666\n",
      "            0         1         2         3    4\n",
      "100  0.583333  0.458333  0.762712  0.708333  2.0\n",
      "101  0.194444  0.125000  0.389831  0.375000  1.0\n",
      "102  0.388889  0.333333  0.593220  0.500000  1.0\n",
      "103  0.944444  0.750000  0.966102  0.875000  2.0\n",
      "104  0.527778  0.583333  0.745763  0.916667  2.0\n",
      "105  0.944444  0.250000  1.000000  0.916667  2.0\n",
      "106  0.388889  0.375000  0.542373  0.500000  1.0\n",
      "107  0.694444  0.500000  0.830508  0.916667  2.0\n",
      "108  0.416667  0.333333  0.694915  0.958333  2.0\n",
      "109  0.555556  0.291667  0.661017  0.708333  2.0\n",
      "110  0.333333  0.125000  0.508475  0.500000  1.0\n",
      "111  0.611111  0.500000  0.694915  0.791667  2.0\n",
      "112  0.305556  0.583333  0.118644  0.041667  0.0\n",
      "113  0.472222  0.416667  0.644068  0.708333  2.0\n",
      "114  0.666667  0.458333  0.576271  0.541667  1.0\n",
      "115  0.555556  0.375000  0.779661  0.708333  2.0\n",
      "116  0.222222  0.708333  0.084746  0.125000  0.0\n",
      "117  0.527778  0.333333  0.644068  0.708333  2.0\n",
      "118  0.500000  0.333333  0.627119  0.458333  1.0\n",
      "119  0.500000  0.416667  0.610169  0.541667  1.0\n",
      "120  0.222222  0.625000  0.067797  0.041667  0.0\n",
      "121  0.944444  0.416667  0.864407  0.916667  2.0\n",
      "122  0.444444  0.500000  0.644068  0.708333  1.0\n",
      "123  0.111111  0.500000  0.101695  0.041667  0.0\n",
      "124  0.416667  0.291667  0.491525  0.458333  1.0\n",
      "[[ -8.13802083   6.91731771 -13.24152542 -14.6484375 ]\n",
      " [ -4.8828125   -7.73111979   1.65519068  -1.22070312]\n",
      " [  7.05295139 -11.39322917  12.08289195  12.61393229]]\n",
      "[2. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 2. 1. 1. 1.]\n",
      "0.4\n",
      "            0         1         2         3    4\n",
      "125  0.361111  0.416667  0.525424  0.500000  1.0\n",
      "126  0.222222  0.750000  0.084746  0.083333  0.0\n",
      "127  0.166667  0.166667  0.389831  0.375000  1.0\n",
      "128  0.416667  0.291667  0.694915  0.750000  2.0\n",
      "129  0.250000  0.583333  0.067797  0.041667  0.0\n",
      "130  0.805556  0.416667  0.813559  0.625000  2.0\n",
      "131  0.222222  0.625000  0.067797  0.083333  0.0\n",
      "132  0.361111  0.375000  0.440678  0.500000  1.0\n",
      "133  0.666667  0.416667  0.711864  0.916667  2.0\n",
      "134  0.305556  0.583333  0.084746  0.125000  0.0\n",
      "[[ -1.65180046  10.73670301 -12.76645781 -13.62735382]\n",
      " [ -6.0566017   -9.49785266  -0.16797971   1.65180046]\n",
      " [  6.33190177  -7.43310208   9.40686365   9.08490254]]\n",
      "[2. 0. 2. 0. 0. 2. 2. 0. 2. 2. 0. 2. 0. 0. 0.]\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import linear_model\n",
    "import pandas as pd\n",
    "\n",
    "chunksize = 25\n",
    "for chunk in pd.read_csv('iris_train.csv', header=None, chunksize=chunksize): #loading the csv file 25 chunks per iterations\n",
    "    train_sub = chunk\n",
    "    print(train_sub)\n",
    "    Y=train_sub[4]\n",
    "    X=train_sub.drop([4], axis=1)\n",
    "    clf = linear_model.SGDClassifier()\n",
    "    clf.partial_fit(X,Y, classes=np.unique(Y))\n",
    "    print(clf.coef_)\n",
    "    clf.predict(test)\n",
    "    pred=clf.predict(test)\n",
    "    print(pred)\n",
    "    print(accuracy_score(pred, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f849a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
